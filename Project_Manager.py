import streamlit as st
import os
import re
import json
import sqlite3
from datetime import datetime
from utils.parser_docx import parse_docx_status
from utils.parser_pdf import parse_pdf_status
from utils.parser_pptx import parse_pptx_status
from utils.parser_vtt import parse_vtt_status
from utils.parser_email import parse_email_status
from pipeline.compare import compare_kpis
from pipeline.risk_detect import detect_risks
from utils.openai_client import ask_gpt
import hashlib
import pandas as pd
import math


# ---------- INIT DB ----------
DB_PATH = "data/project_data.db"
os.makedirs("data", exist_ok=True)

conn = sqlite3.connect(DB_PATH, check_same_thread=False)

cursor = conn.cursor()

cursor.execute('''
CREATE TABLE IF NOT EXISTS projects (
    id TEXT PRIMARY KEY,
    name TEXT,
    issuer TEXT,
    start_date TEXT,
    summary TEXT,
    contacts TEXT,
    tags TEXT,
    rfp_file TEXT,
    status TEXT DEFAULT 'active',
    created_at TEXT
)
''')

cursor.execute('''
CREATE TABLE IF NOT EXISTS files (
    id TEXT PRIMARY KEY,
    project_id TEXT,
    filename TEXT,
    file_type TEXT,
    snapshot_date TEXT,
    uploaded_at TEXT,
    snapshot_data TEXT, -- renamed from llm_output
    FOREIGN KEY(project_id) REFERENCES projects(id)
)

''')

# Create the risk_cache table if it doesn't exist
cursor.execute("""
CREATE TABLE IF NOT EXISTS risk_cache (
    project_id TEXT,
    current_date TEXT,
    previous_date TEXT,
    snapshot_pair_hash TEXT PRIMARY KEY,
    risk_json TEXT,
    generated_at TEXT
)
""")

conn.commit()

# ---------- PAGE UI ----------
st.set_page_config(page_title="üóÇÔ∏è Project Manager Hub", layout="wide")
st.title("üß© Project Manager")
tabs = st.tabs([
    "‚ûï Initialize Project",
    "üìÅ Upload File",
    "üìÇ View Uploaded Files",
    "üìä Upload Excel Snapshot"
])


# ---------- TAB 1: Project Initialization ----------
with tabs[0]:
    st.subheader("‚ûï Create New Project")
    with st.form("create_project_form"):
        name = st.text_input("Project Name")
        issuer = st.text_input("Issuer / Client")
        start_date = st.date_input("Start Date").strftime("%Y-%m-%d")
        summary = st.text_area("Project Summary")
        contacts = st.text_area("Key Contacts (Name, Role, Email)")
        tags = st.text_input("Tags (comma-separated)")
        status = st.selectbox("Project Status", ["active", "completed", "archived"])
        submitted = st.form_submit_button("‚úÖ Create Project")

        if submitted:
            project_id = re.sub(r'[^a-zA-Z0-9_]', '_', name.strip().lower())
            cursor.execute("""
                INSERT OR REPLACE INTO projects
                (id, name, issuer, start_date, summary, contacts, tags, status, created_at)
                VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)
            """, (
                project_id, name, issuer, start_date, summary,
                contacts, tags, status, datetime.now().isoformat()
            ))
            conn.commit()
            st.success(f"‚úÖ Project '{name}' initialized.")

# ---------- TAB 2: Upload and Parse File ----------
with tabs[1]:

    st.subheader("üìÑ Upload Project File")

    # --- Project Selector ---
    project_options = cursor.execute("SELECT id, name FROM projects").fetchall()

    if not project_options:
        st.warning("No projects found. Please create a project first.")
        st.stop()

    project_map = {f"{name} ({pid})": pid for pid, name in project_options}
    selected_label = st.selectbox("Select Project", list(project_map.keys()))
    selected_project = project_map.get(selected_label)

    if not selected_project:
        st.warning("Please select a project to proceed.")
        st.stop()

    # --- Upload File ---
    uploaded_files = st.file_uploader(
        "Upload project files (folder or multiple)", 
        type=["docx", "pdf", "pptx", "vtt", "eml", "msg"], 
        accept_multiple_files=True
    )

    if uploaded_files:
        for uploaded_file in uploaded_files:
            file_type = uploaded_file.name.split(".")[-1].lower()
            st.markdown(f"---\n### üìÑ Processing: `{uploaded_file.name}`")

            try:
                if file_type == "docx":
                    result = parse_docx_status(uploaded_file)
                elif file_type == "pdf":
                    result = parse_pdf_status(uploaded_file)
                elif file_type == "pptx":
                    result = parse_pptx_status(uploaded_file)
                elif file_type == "vtt":
                    result = parse_vtt_status(uploaded_file)
                elif file_type in ["eml", "msg"]:
                    result = parse_email_status(uploaded_file)
                else:
                    st.error("Unsupported file type.")
                    continue
            except Exception as e:
                st.error(f"‚ùå Parsing failed: {e}")
                continue

            raw_text = result.get("raw_text", "")
            parsed = result.get("parsed", {})
            report_date = parsed.get("report_date") or datetime.today().strftime("%Y-%m-%d")

            from utils.openai_client import ask_gpt
            import json

            # --- Fetch prior snapshot ---
            cursor.execute("""
                SELECT llm_output FROM files
                WHERE project_id = ? AND report_date < ?
                ORDER BY report_date DESC
                LIMIT 1
            """, (selected_project, report_date))
            row = cursor.fetchone()
            previous_llm_output = json.loads(row[0]) if row else {}

            # --- Ask GPT to revise prior snapshot using new document ---
            gpt_prompt = f"""
You are a project analyst. You are given two inputs:
1. The current full project snapshot (in JSON format)
2. A new document containing updated project information

Your job is to revise the current snapshot. Overwrite any values in the JSON **only if the new document explicitly updates or corrects them**. You may also **append any new risks, issues, deliverables, or schedule items** mentioned in the document, if they do not already exist in the JSON.

DO NOT delete or null out anything unless the document explicitly says it is removed. If the document provides no new information on a field, leave the previous value as is.

Only return a valid JSON object.

--- CURRENT PROJECT SNAPSHOT ---
{json.dumps(previous_llm_output, indent=2)}

--- NEW DOCUMENT TEXT ---
{raw_text.strip()[:12000]}
"""
            revised_snapshot = ask_gpt(gpt_prompt)

            # --- Mandatory JSON reformat pass ---
            example_json_format = {
                "report_date": "2025-07-31",
                "source": "document",
                "summary": None,
                "kpis": {
                    "budget": None,
                    "timeline": None,
                    "scope": None,
                    "client_sentiment": None,
                    "allotted_budget": None,
                    "spent_budget": None,
                    "remaining_budget": None,
                    "percent_spent": None
                },
                "schedule": [
                    {
                        "Task ID": "",
                        "Task Name": "",
                        "Description": "",
                        "Assigned To": "",
                        "Start Date": "",
                        "End Date": "",
                        "Duration (Days)": 0,
                        "Status": "",
                        "Dependencies": ""
                    }
                ],
                "issues": [
                    {
                        "Issue #": "",
                        "Issue Creation Date": "",
                        "Issue Category": "",
                        "Issue Detail": "",
                        "Recommended Action": "",
                        "Owner": "",
                        "Status": "",
                        "Due Date": "",
                        "Resolution": ""
                    }
                ],
                "risks": [
                    {
                        "ID": "",
                        "Division": "",
                        "Task Area": "",
                        "Risk Name": "",
                        "Risk Description": "",
                        "Risk Category": "",
                        "Probability Rating": 0,
                        "Impact Rating": 0,
                        "Risk Rating": 0,
                        "Impact If Not Mitigated": "",
                        "Action/Mitigation Strategy": "",
                        "Mitigation Owner(s)": "",
                        "Action Taken?": "",
                        "Date Identified": ""
                    }
                ],
                "deliverables": [
                    {
                        "Deliverable": "",
                        "Status": "",
                        "Start Date": "",
                        "Date Due": ""
                    }
                ],
                "budget_details": [
                    {
                        "Category": "",
                        "Allotted Budget": 0.0,
                        "Spent Budget": 0.0,
                        "Remaining Budget": 0.0,
                        "Percent Spent": 0.0,
                        "Notes": None
                    }
                ]
            }

            format_prompt = f"""
Please take the following LLM-generated output and reformat it as a valid JSON string only. It must match the following structure:

{json.dumps(example_json_format, indent=2)}

--- INPUT ---
{revised_snapshot}
"""
            try:
                structured = json.loads(ask_gpt(format_prompt))
            except:
                st.error("Failed to parse final JSON output. Please check the formatting.")
                continue

            # Show preview
            st.subheader("üìå Parsed Preview")
            st.json(structured)
            with st.expander("üßæ Raw Text Preview", expanded=False):
                st.text(raw_text[:5000] if raw_text else "No raw text found.")

            # Save button
            if st.button(f"üíæ Save {uploaded_file.name}", key=f"save_{uploaded_file.name}"):
                file_id = f"{selected_project}_{report_date}"
                cursor.execute("""
                    INSERT OR REPLACE INTO files
                    (id, project_id, filename, file_type, report_date, uploaded_at, raw_text, metadata, llm_output)
                    VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)
                """, (
                    file_id, selected_project, uploaded_file.name, file_type,
                    report_date, datetime.now().isoformat(),
                    raw_text, json.dumps(parsed), json.dumps(structured)
                ))
                conn.commit()
                st.success(f"‚úÖ `{uploaded_file.name}` saved to project.")



# ---------- TAB 3: View Uploaded Files ----------
with tabs[2]:
    st.subheader("üìÇ Files Uploaded for Project")

    if not project_options:
        st.warning("No projects found. Please create a project first.")
        st.stop()

    selected_label_view = st.selectbox("Select Project to View Files", list(project_map.keys()), key="view_project_select")
    selected_project_view = project_map.get(selected_label_view)

    if not selected_project_view:
        st.warning("Please select a project to proceed.")
        st.stop()

    files = cursor.execute("""
        SELECT filename, report_date, uploaded_at, llm_output
        FROM files
        WHERE project_id = ?
        ORDER BY uploaded_at DESC
    """, (selected_project_view,)).fetchall()

    if not files:
        st.info("No files uploaded for this project yet.")
    else:
        for filename, report_date, uploaded_at, llm_output_json in files:
            st.markdown(f"### üìÅ `{filename}`")
            st.markdown(f"- üóìÔ∏è Report Date: **{report_date}**")
            st.markdown(f"- ‚è±Ô∏è Uploaded At: `{uploaded_at}`")

            if llm_output_json and llm_output_json.strip() not in ("", "null", "{}"):
                try:
                    llm_output = json.loads(llm_output_json)
                    with st.expander("üìå Parsed Snapshot", expanded=False):
                        st.json(llm_output)
                except Exception as e:
                    st.error(f"Error reading llm_output: {e}")
            else:
                st.info("No snapshot available for this file.")


# ---------- TAB 4: Upload Excel Snapshot ----------
with tabs[3]:
    st.subheader("üìä Upload Excel-Based Project Snapshot")

    with st.form("upload_excel_form"):
        uploaded_file = st.file_uploader("Upload Excel File (.xlsx)", type=["xlsx"])
        submitted = st.form_submit_button("üìÖ Upload Snapshot")

    if submitted and uploaded_file:
        try:
            xl = pd.ExcelFile(uploaded_file)

            # --- Title Page: Basic Project Info ---
            title_df = xl.parse("Contents", header=None)

            name = title_df.iloc[2, 1]       # B3
            issuer = title_df.iloc[3, 1]     # B4
            start_date = title_df.iloc[4, 1] # B5
            summary = title_df.iloc[5, 1]    # B6
            tags = title_df.iloc[9, 1]       # B10
            status = title_df.iloc[1, 1]     # B2
            created_at = datetime.now().isoformat()

            if not name:
                st.error("‚ùå 'Project Name' is required in the Title Page.")
                st.stop()

            project_id = name.strip().lower().replace(" ", "_")

            # --- Contacts Sheet: Flexible Multi-Row Contacts ---
            df_contacts = xl.parse("Contents", header=None)
            df_clean = df_contacts.iloc[20:, 0:4]  # From row 21 down, columns A‚ÄìD
            df_clean = df_clean.dropna(how="all")
            df_clean = df_clean.dropna(subset=[1])  # Require Name
            df_clean.columns = ["Role", "Name", "Organization", "Email"]
            contacts_list = df_clean.to_dict(orient="records")
            contacts_json = json.dumps(contacts_list)

           # --- Check if Project Already Exists ---
            cursor.execute("SELECT id FROM projects WHERE id = ?", (project_id,))
            existing = cursor.fetchone()

            if existing:
                selected_project_id = project_id
                st.success(f"‚úÖ Snapshot will be added to existing project '{name}'.")
            else:
                cursor.execute("""
                    INSERT INTO projects
                    (id, name, issuer, start_date, summary, contacts, tags, status, created_at)
                    VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)
                """, (
                    project_id, name, issuer, start_date, summary,
                    contacts_json, tags, status, created_at
                ))
                conn.commit()
                selected_project_id = project_id
                st.success(f"üÜï Project '{name}' created and initialized.")

            # --- Helper to Get Previous Snapshot Sentiment ---
            def get_previous_client_sentiment(cursor, project_id, current_uploaded_at):
                """
                Returns the most recent client_sentiment value from the previous snapshot
                BEFORE the current_uploaded_at timestamp.
                """
                cursor.execute("""
                    SELECT llm_output 
                    FROM files 
                    WHERE project_id = ? AND uploaded_at < ?
                    ORDER BY uploaded_at DESC
                    LIMIT 1
                """, (project_id, current_uploaded_at))
                
                row = cursor.fetchone()

                if row:
                    try:
                        output = json.loads(row[0])
                        return output.get("kpis", {}).get("client_sentiment", "Positive")
                    except:
                        return "Positive"
                
                return "Positive"



            # --- Budget Sheet: Extract Budget Data ---
            try:
                budget_df = xl.parse("Budget", header=1)  # Header is on row 2 (index 1)
                budget_df = budget_df.dropna(how="all")

                expected_budget_cols = [
                    "Category", "Allotted Budget", "Spent Budget",
                    "Remaining Budget", "Percent Spent", "Notes"
                ]

                if all(col in budget_df.columns for col in expected_budget_cols):
                    # Convert budget numbers to floats
                    for col in ["Allotted Budget", "Spent Budget", "Remaining Budget", "Percent Spent"]:
                        budget_df[col] = pd.to_numeric(budget_df[col], errors="coerce")

                    budget_details = budget_df.to_dict(orient="records")

                    # Safely extract top-level budget KPIs from the "Total" row
                    total_row = budget_df[
                        budget_df["Category"].astype(str).str.lower() == "total"
                    ]

                    if not total_row.empty:
                        total_row = total_row.iloc[0]
                        allotted = float(total_row["Allotted Budget"]) if pd.notnull(total_row["Allotted Budget"]) else None
                        spent = float(total_row["Spent Budget"]) if pd.notnull(total_row["Spent Budget"]) else None
                        remaining = float(total_row["Remaining Budget"]) if pd.notnull(total_row["Remaining Budget"]) else None
                        percent_spent = float(total_row["Percent Spent"]) if pd.notnull(total_row["Percent Spent"]) else None
                        st.success(f"‚úÖ Parsed budget sheet with {len(budget_details)} categories.")
                    else:
                        st.warning("‚ö†Ô∏è Could not find 'Total' row in Budget sheet.")
                        allotted = spent = remaining = percent_spent = None

                else:
                    st.warning("‚ö†Ô∏è Budget sheet missing expected columns. Skipping budget parsing.")
                    allotted = spent = remaining = percent_spent = None
                    budget_details = []

            except Exception as e:
                st.warning(f"‚ö†Ô∏è Could not parse Budget sheet: {e}")
                allotted = spent = remaining = percent_spent = None
                budget_details = []



            # --- Schedule Sheet: Extract Task Data ---
            try:
                schedule_df = xl.parse("Schedule")
                schedule_df = schedule_df.dropna(how="all")

                expected_columns = [
                    "Task ID", "Task Name", "Description", "Assigned To",
                    "Start Date", "End Date", "Duration (Days)", "Status", "Dependencies"
                ]

                if all(col in schedule_df.columns for col in expected_columns):
                    # Convert datetime columns to ISO string format
                    schedule_df["Start Date"] = pd.to_datetime(schedule_df["Start Date"], errors="coerce").dt.strftime("%Y-%m-%d")
                    schedule_df["End Date"] = pd.to_datetime(schedule_df["End Date"], errors="coerce").dt.strftime("%Y-%m-%d")

                    # Extract only the expected columns, as list of dicts
                    schedule = schedule_df[expected_columns].to_dict(orient="records")
                    st.success(f"‚úÖ Parsed {len(schedule)} tasks from the Schedule sheet.")
                else:
                    st.warning("‚ö†Ô∏è Schedule sheet missing expected columns. Skipping task parsing.")
                    schedule = []
            except Exception as e:
                st.warning(f"‚ö†Ô∏è Failed to parse Schedule sheet: {e}")
                schedule = []
            
            # --- Issue Log Sheet: Extract Issue Data ---
            try:
                issue_df = xl.parse("Issue Log")
                issue_df = issue_df.dropna(how="all")

                expected_issue_columns = [
                    "Issue #", "Issue Creation Date", "Issue Category", "Issue Detail",
                    "Recommended Action", "Owner", "Status", "Due Date", "Resolution"
                ]

                if all(col in issue_df.columns for col in expected_issue_columns):
                    # Clean date columns to ensure valid ISO strings
                    def clean_dates(df, cols):
                        for col in cols:
                            df[col] = pd.to_datetime(df[col], errors="coerce")
                            df[col] = df[col].dt.strftime('%Y-%m-%d')  # force clean ISO format
                        return df

                    issue_df = clean_dates(issue_df, ["Issue Creation Date", "Due Date"])

                    issues = issue_df[expected_issue_columns].to_dict(orient="records")
                    st.success(f"‚úÖ Parsed {len(issues)} issues from the Issue Log.")
                else:
                    st.warning("‚ö†Ô∏è Issue Log sheet missing expected columns. Skipping issue parsing.")
                    issues = []
            except Exception as e:
                st.warning(f"‚ö†Ô∏è Failed to parse Issue Log sheet: {e}")
                issues = []


            # --- Deliverable Status Sheet ---
            try:
                deliverables_df = xl.parse("Deliverable Status")
                deliverables_df = deliverables_df.dropna(how="all")

                expected_deliverable_cols = [
                    "Deliverable", "Status", "Start Date", "Date Due"
                ]

                if all(col in deliverables_df.columns for col in expected_deliverable_cols):
                    # Ensure dates are parsed and formatted as ISO strings
                    def clean_dates(df, cols):
                        for col in cols:
                            df[col] = pd.to_datetime(df[col], errors="coerce")
                            df[col] = df[col].dt.strftime('%Y-%m-%d')  # Clean format
                        return df

                    deliverables_df = clean_dates(deliverables_df, ["Start Date", "Date Due"])

                    deliverables = deliverables_df[expected_deliverable_cols].to_dict(orient="records")
                    st.success(f"‚úÖ Parsed {len(deliverables)} deliverables from Deliverable Status sheet.")
                else:
                    st.warning("‚ö†Ô∏è Deliverable Status sheet missing expected columns. Skipping deliverable parsing.")
                    deliverables = []
            except Exception as e:
                st.warning(f"‚ö†Ô∏è Could not parse Deliverable Status sheet: {e}")
                deliverables = []
            # --- Helper Function to Clean NaNs for JSON ---
            def clean_nans(obj):
                if isinstance(obj, dict):
                    return {k: clean_nans(v) for k, v in obj.items()}
                elif isinstance(obj, list):
                    return [clean_nans(v) for v in obj]
                elif isinstance(obj, float) and math.isnan(obj):
                    return None
                return obj
            
           # --- Risk Assessment Sheet: Extract Risk Data ---
            try:
                risk_df = xl.parse("Risk Assessment")

                expected_risk_cols = [
                    "ID", "Division", "Task Area", "Risk Name and Description",
                    "Risk Category", "Probability Rating", "Impact Rating", "Risk Rating",
                    "Impact If Not Mitigated", "Action/Mitigation Strategy", "Mitigation Owner(s)",
                    "Action Taken?", "Date Identified"
                ]

                if all(col in risk_df.columns for col in expected_risk_cols):
                    # Convert Risk Rating to numeric and drop if 0 or NaN
                    risk_df["Risk Rating"] = pd.to_numeric(risk_df["Risk Rating"], errors="coerce")
                    risk_df = risk_df[risk_df["Risk Rating"] > 0]

                    # Clean dates
                    risk_df["Date Identified"] = pd.to_datetime(risk_df["Date Identified"], errors="coerce")
                    risk_df["Date Identified"] = risk_df["Date Identified"].dt.strftime('%Y-%m-%d')

                    risks = risk_df[expected_risk_cols].to_dict(orient="records")
                    st.success(f"‚úÖ Parsed {len(risks)} risk(s) from Risk Assessment sheet.")
                else:
                    st.warning("‚ö†Ô∏è Risk Assessment sheet missing expected columns. Skipping risk parsing.")
                    risks = []
            except Exception as e:
                st.warning(f"‚ö†Ô∏è Failed to parse Risk Assessment sheet: {e}")
                risks = []



            # --- Helper to Evaluate Timeline with GPT ---
            def assess_timeline_kpi(schedule, deliverables):
                import json
                from utils.openai_client import ask_gpt

                prompt = f"""
            You are a project health evaluator.
            Based on today's date, a list of schedule tasks and deliverables with start/end dates and statuses,
            determine if the project is "On Track" or "At-Risk". 
            Focus only on missed or overdue tasks.
            Please only return the phrase "On Track" or At-Risk," nothing else. 
            Please do not explain your reasoning, provide any commentary, etc. 
            Your reply should be one of the two given phrases.

            Schedule:
            {json.dumps(schedule, indent=2)}

            Deliverables:
            {json.dumps(deliverables, indent=2)}
            """
                response = ask_gpt(prompt)
                if "at-risk" in response.lower():
                    return "At-Risk"
                return "On Track"

            # --- Helper to Evaluate Scope with GPT ---
            def assess_scope_kpi(schedule, deliverables, issues):
                import json
                from utils.openai_client import ask_gpt

                prompt = f"""
            You are a scope change evaluator.
            Based on a project's current schedule, deliverables, and logged issues,
            determine whether the project scope has remained "Unchanged", has "Narrowed", or has "Widened".
            Return only one of those three phrases. Do not explain your reasoning.

            Schedule:
            {json.dumps(schedule, indent=2)}

            Deliverables:
            {json.dumps(deliverables, indent=2)}

            Issues:
            {json.dumps(issues, indent=2)}
            """
                response = ask_gpt(prompt)
                lowered = response.lower()
                if "narrow" in lowered:
                    return "Scope Narrowed"
                elif "wide" in lowered:
                    return "Scope Widened"
                return "Unchanged"
            
            # --- Construct llm_output Snapshot JSON ---
            try:
                allotted = float(total_row["Allotted Budget"])
                spent = float(total_row["Spent Budget"])
                remaining = float(total_row["Remaining Budget"])
                percent_spent = float(total_row["Percent Spent"])
            except:
                allotted = spent = remaining = percent_spent = None
            timeline_kpi = assess_timeline_kpi(schedule, deliverables)
            current_uploaded_at = datetime.now().isoformat()
            sentiment = get_previous_client_sentiment(cursor, selected_project_id, current_uploaded_at)
            scope_kpi = assess_scope_kpi(schedule, deliverables, issues)

            llm_output = {
                "report_date": datetime.now().strftime("%Y-%m-%d"),
                "source": "excel",
                "summary": summary,
                "kpis": {
                    "budget": f"${allotted:,.0f} ({percent_spent:.0f}% used)" if allotted is not None else None,
                    "timeline": timeline_kpi,
                    "scope": scope_kpi,
                    "client_sentiment": sentiment,
                    "allotted_budget": allotted,
                    "spent_budget": spent,
                    "remaining_budget": remaining,
                    "percent_spent": percent_spent
                },
                "schedule": schedule,
                "issues": issues,
                "risks": risks,
                "deliverables": deliverables,
                "budget_details": budget_details
            }

            # --- Clean NaNs to avoid JSON serialization issues ---
            llm_output_clean = clean_nans(llm_output)

            # --- Save to DB ---
            file_id = f"{selected_project_id}_{datetime.now().strftime('%Y%m%d%H%M%S')}"
            cursor.execute("""
                INSERT INTO files (id, project_id, filename, file_type, report_date, uploaded_at, raw_text, metadata, llm_output)
                VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)
            """, (
                file_id, selected_project_id, uploaded_file.name, "excel",
                llm_output_clean["report_date"], datetime.now().isoformat(),
                "", "", json.dumps(llm_output_clean)
            ))
            conn.commit()
            st.success("‚úÖ Snapshot saved and Excel data parsed.")

        except Exception as e:
            st.error(f"‚ùå Failed to process Excel file: {e}")



